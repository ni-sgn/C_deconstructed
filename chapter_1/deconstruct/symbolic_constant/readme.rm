What is a symbolic constant?

Source code is written in certain set of characters, wether it
is utf or ascii, these characters can't be changed, but we can
define other set of characters that then will be substituted 
with other set of characters.
These are called 'symbolic constants'.

To achieve this in C, one should use something called
preprocessor but before we talk about it, we have to take
a detour.

What is a programming language?
I don't really know at this point. But I have some
idea about what it is not.

Probably, on the lowest level programming language is 
just a formal language with its syntax and grammar rules.
well-defined. 
But what does that have to do with computers?
On this level, nothing... At least in my mind...
We can write expressions in this language on the paper
and check the syntax manually...

But, we want to use this formal, well-defined language
as a tool to communicate with machines.
Then things got complicated for me.
Implementation of a language on a machine is a complex task...
That's for sure...

"Machines only understand 1s and 0s" - And the confusion is born.
What is this supposed to mean? Where are this 1s and 0s coming from?
My answer to that is circuits and trickery. plus the math of 
computation and logic..
Humans just use the properties of natural order to get the results they
wish for... We also have a name for that, it's called Engineering.
Electrical Engineering in this case is what comes up with tricky circuits.
This circuits have input and when charge with certaing voltage approaches. Certain
pathways are activated, like valves being open, which leads to certain desired output.
Behind this there's theory, which says that with certain, small amount of functions
we can describe any computable function.
Therefore with sequences and compositions of these 'instructions' we have an ability 
to describe anything computable.

Therefore the CPU accepts certain inputs and rejects others, which means that we
already have a language. This language is perfect for computers but not so for humans.
Translating it manually is very time consuming. We need a middle ground, such that
it will be easy for humans to understand and also easy to translate it to a machine language.

We don't want to translate this higher level language into the machine language. Computer must be
able to do it.  
But again how is it implemented?
There must be a program which takes the 'higher language' as an input and generates
'machine language' as an output. 
I think this is also a fancy computational theory. Probably simulating 
Deterministic Finite Automata, this 'higher language' can be parsed, syntactically checked
and accepted, then mapped to 'machine language' instructions.

Humans are using characters or letters to construct words, sentences and ideas.
Computers must have a way to represent a character.
For that, sequences of 1s and 0s, which computer understands, are mapped to characters.
This mapping can be of many kinds, therefore standard is needed. That's UTF-8 and ASCII.
That's how a machine understands letters...

When there are codes for letters, and a thoeritical construct of a formal language,
what's left is a program, algorithm, that takes the  'high level language' as an input 
and translates it to the 'machine language' as an output.
This program must be written and fed to the machine as 'machine language' at first.
Now it is possible to feed that program with a sequance of letters and it will either
accept it or reject. 
And from that we can build infinite amount of complexity...

And this 'Infinite amount of complexity' once again is the root of 
all kinds of confusion...



